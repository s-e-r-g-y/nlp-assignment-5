{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f52e4050a50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read and process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(name):\n",
    "    f = open(path + name + '.txt', 'r')\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    sentence = []\n",
    "    tag_sentence = []\n",
    "    \n",
    "    for line in lines[2:]:\n",
    "        if line == '\\n':           \n",
    "            \n",
    "            if len(sentence) > 0:\n",
    "                result.append((sentence, tag_sentence))\n",
    "            \n",
    "                sentence = []\n",
    "                tag_sentence = []\n",
    "            \n",
    "            continue\n",
    "            \n",
    "        words = line.strip().split()\n",
    "        \n",
    "        if words[0] == '-DOCSTART-':\n",
    "            continue\n",
    "        \n",
    "        sentence.append(words[0])\n",
    "        tag_sentence.append(words[3])\n",
    "            \n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_data('train')\n",
    "dev = get_data('dev')\n",
    "test = get_data('test')\n",
    "\n",
    "dataset = train + dev + test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement 3 strategies for loading the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_name = 'glove.6B.100d.txt'\n",
    "\n",
    "def loadGloveModel(file):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(file,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "gmodel = loadGloveModel(path + embeddings_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "tag_vocab = {}\n",
    "for sent, tags in dataset:\n",
    "    for word in sent:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "    for tag in tags:\n",
    "        if tag not in tag_vocab:\n",
    "            tag_vocab[tag] = len(tag_vocab)\n",
    "\n",
    "vocab['pad'] = len(vocab)\n",
    "            \n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC': 5,\n",
       " 'B-MISC': 2,\n",
       " 'B-ORG': 0,\n",
       " 'B-PER': 3,\n",
       " 'I-LOC': 8,\n",
       " 'I-MISC': 7,\n",
       " 'I-ORG': 6,\n",
       " 'I-PER': 4,\n",
       " 'O': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.a.** Load the embeddings for original capitalization of words. If embedding for this word doesn’t exists, associate it with UNKNOWN embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk = gmodel['unk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodel_strategy_1 = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, index in vocab.items():\n",
    "    if word in gmodel:\n",
    "        gmodel_strategy_1[index,:] = gmodel[word]\n",
    "    else:\n",
    "        gmodel_strategy_1[index,:] = unk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.b.** load the embeddings for lowercased capitalization of words. If embedding for this lowercased word doesn’t exists, associate it with UNKNOWN embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodel_strategy_2 = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, index in vocab.items():\n",
    "    word_lower = word.lower()\n",
    "    if word_lower in gmodel:\n",
    "        gmodel_strategy_2[index,:] = gmodel[word_lower]\n",
    "    else:\n",
    "        gmodel_strategy_2[index,:] = unk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.c.** load the embeddings for original capitalization of words. If embedding for this word doesn't exists, try to find the embedding for lowercased version and associate it to the word with original capitalization. Otherwise, associate it with UNKNOWN embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodel_strategy_3 = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, index in vocab.items():\n",
    "    word_lower = word.lower()\n",
    "    if word in gmodel:\n",
    "        gmodel_strategy_3[index,:] = gmodel[word]\n",
    "    elif word_lower in gmodel:\n",
    "        gmodel_strategy_3[index,:] = gmodel[word_lower]\n",
    "    else:\n",
    "        gmodel_strategy_3[index,:] = unk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implement training on batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "def prepare_batch(batch_sentences, vocab):\n",
    "    batch_max_len = max([len(s[0]) for s in batch_sentences])\n",
    "\n",
    "    #prepare a numpy array with the data, initializing the data with 'PAD' \n",
    "    #and all labels with -1; initializing labels to -1 differentiates tokens \n",
    "    #with tags from 'PAD' tokens\n",
    "    batch_data = vocab['pad']*np.ones((len(batch_sentences), batch_max_len))\n",
    "    batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n",
    "\n",
    "    #copy the data to the numpy array\n",
    "    for j in range(len(batch_sentences)):\n",
    "        cur_len = len(batch_sentences[j][0])\n",
    "        batch_data[j][:cur_len] = prepare_sequence(batch_sentences[j][0], vocab)\n",
    "        batch_labels[j][:cur_len] = prepare_sequence(batch_sentences[j][1], tag_vocab)\n",
    "\n",
    "    #since all data are indices, we convert them to torch LongTensors\n",
    "    batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "\n",
    "    return batch_data, batch_labels\n",
    "\n",
    "    \n",
    "# get_batch(train[:10], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_NER(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, embedding_matrix, tagset_size):\n",
    "        super(LSTM_NER, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(embedding_matrix.shape[0], embedding_dim).\\\n",
    "            from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float), freeze=True)            \n",
    "        self.word_embeddings.weight.requires_grad = False\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, dropout=0.2, bidirectional=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.fc = nn.Linear(2*hidden_dim, tagset_size)\n",
    "        \n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        \n",
    "        lstm_out = lstm_out.view(-1, lstm_out.shape[2])\n",
    "        \n",
    "        fc_out = self.fc(lstm_out)\n",
    "        \n",
    "        tag_scores = F.log_softmax(fc_out, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1571, -2.1610, -2.1721,  ..., -2.3158, -2.2886, -2.1345],\n",
      "        [-2.1914, -2.1702, -2.1949,  ..., -2.3141, -2.2587, -2.1367],\n",
      "        [-2.1551, -2.1499, -2.1952,  ..., -2.3052, -2.2815, -2.1415],\n",
      "        ...,\n",
      "        [-2.1512, -2.1951, -2.1636,  ..., -2.2718, -2.2741, -2.1767],\n",
      "        [-2.1382, -2.1923, -2.1775,  ..., -2.2673, -2.2758, -2.1879],\n",
      "        [-2.1505, -2.1960, -2.1838,  ..., -2.2717, -2.2569, -2.1819]])\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = 50\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "model = LSTM_NER(EMBEDDING_DIM, HIDDEN_DIM, gmodel_strategy_3, len(tag_vocab))\n",
    "# loss_function = nn.NLLLoss()\n",
    "\n",
    "def loss_function(outputs, labels):\n",
    "    #reshape labels to give a flat vector of length batch_size*seq_len\n",
    "    labels = labels.view(-1)  \n",
    "\n",
    "    #mask out 'PAD' tokens\n",
    "    mask = (labels >= 0).float()\n",
    "\n",
    "    #the number of tokens is the sum of elements in mask\n",
    "    num_tokens = int(torch.sum(mask).data.item())\n",
    "\n",
    "    #pick the values corresponding to labels and multiply by mask\n",
    "    outputs = outputs[range(outputs.shape[0]), labels]*mask\n",
    "\n",
    "    #cross entropy loss for all non 'PAD' tokens\n",
    "    return -torch.sum(outputs)/num_tokens\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "#     inputs = prepare_sequence(train[0][0], vocab)\n",
    "    inputs, labels = prepare_batch(train[:BATCH_SIZE], vocab)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(b_size, data):\n",
    "    batch_sentences = random.sample(data, b_size)\n",
    "    \n",
    "    return prepare_batch(batch_sentences, vocab)\n",
    "\n",
    "\n",
    "def get_accuracy(prediction, target):\n",
    "    index_global = 0\n",
    "    counter = 0\n",
    "    \n",
    "    classes_tp = np.zeros(len(tag_vocab))\n",
    "    classes_fp = np.zeros(len(tag_vocab))\n",
    "    classes_fn = np.zeros(len(tag_vocab))\n",
    "    \n",
    "    for row in target:\n",
    "        for col in row:        \n",
    "            if col < 0:\n",
    "                continue\n",
    "\n",
    "            result = col.item()\n",
    "            \n",
    "            if result == 1:\n",
    "                continue\n",
    "\n",
    "            max_val, pred = prediction[ index_global ].max(0)\n",
    "\n",
    "            if pred == result:\n",
    "                counter += 1\n",
    "                \n",
    "                \n",
    "            if pred == result:\n",
    "                classes_tp[result] += 1\n",
    "            else:\n",
    "                classes_fp[pred] += 1\n",
    "                classes_fn[result] += 1\n",
    "            \n",
    "\n",
    "            index_global += 1\n",
    "\n",
    "#     print(classes_tp)\n",
    "#     print(classes_fp)\n",
    "#     print(classes_fn)\n",
    "\n",
    "    tp = np.sum(classes_tp)\n",
    "    fp = np.sum(classes_fp)\n",
    "    fn = np.sum(classes_fn)\n",
    "\n",
    "    return tp / (tp + fp), tp / (tp + fn)\n",
    "\n",
    "\n",
    "def validate_model(model, dataset, vocab, tag_vocab):\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        size = len(dev)\n",
    "\n",
    "        sentence_in, targets = prepare_batch(dataset, vocab)\n",
    "\n",
    "        prediction = model(sentence_in)\n",
    "        \n",
    "        pr, rc = get_accuracy(prediction, targets)\n",
    "        loss = loss_function(prediction, targets)\n",
    "\n",
    "        return pr, rc, loss.item()\n",
    "\n",
    "\n",
    "# np.round(validate_model(model, test, vocab, tag_vocab), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03fd717ff684fb8a911338e8b278d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=235), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:     0.850249\n",
      "Validate loss:  0.693659\n",
      "Validate accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "steps = len(train) // BATCH_SIZE + 1\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "train_sorted = sorted(train, key=lambda item: (len(item[0])))\n",
    "\n",
    "for epoch in range(5):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    loss_sum = 0\n",
    "    print(\"Epoch: {}\".format(epoch + 1))\n",
    "    for step in tqdm_notebook(range(steps)):\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        \n",
    "        # sentence_in, targets = get_batch(BATCH_SIZE, train)        \n",
    "        sentence_in, targets = prepare_batch(train_sorted[step * BATCH_SIZE: (step+1) * BATCH_SIZE], vocab)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_sum += loss\n",
    "        \n",
    "    avg_epoch_loss = np.round((loss_sum / steps).item(), 6)\n",
    "    dev_epoch_pr, dev_epoch_rc, dev_epoch_loss = np.round(validate_model(model, dev, vocab, tag_vocab), 6)\n",
    "    \n",
    "    print(\"Train loss:     {}\\nValidate loss:  {}\\nValidate accuracy: {}\".\\\n",
    "    format(avg_epoch_loss, dev_epoch_loss, dev_epoch_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_model(model, test, vocab, tag_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
