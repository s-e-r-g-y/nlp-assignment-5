{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2d48088a50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read and process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(name):\n",
    "    f = open(path + name + '.txt', 'r')\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    sentence = []\n",
    "    tag_sentence = []\n",
    "    \n",
    "    for line in lines[2:]:\n",
    "        if line == '\\n':           \n",
    "            \n",
    "            if len(sentence) > 0:\n",
    "                result.append((sentence, tag_sentence))\n",
    "            \n",
    "                sentence = []\n",
    "                tag_sentence = []\n",
    "            \n",
    "            continue\n",
    "            \n",
    "        words = line.strip().split()\n",
    "        \n",
    "        if words[0] == '-DOCSTART-':\n",
    "            continue\n",
    "        \n",
    "        sentence.append(words[0])\n",
    "        tag_sentence.append(words[3])\n",
    "            \n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_data('train')\n",
    "dev = get_data('dev')\n",
    "test = get_data('test')\n",
    "\n",
    "dataset = train + dev + test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement 3 strategies for loading the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_name = 'glove.6B.100d.txt'\n",
    "\n",
    "def loadGloveModel(file):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(file,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "gmodel = loadGloveModel(path + embeddings_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "tag_vocab = {}\n",
    "for sent, tags in dataset:\n",
    "    for word in sent:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "    for tag in tags:\n",
    "        if tag not in tag_vocab:\n",
    "            tag_vocab[tag] = len(tag_vocab)\n",
    "\n",
    "vocab['pad'] = len(vocab)\n",
    "            \n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC': 5,\n",
       " 'B-MISC': 2,\n",
       " 'B-ORG': 0,\n",
       " 'B-PER': 3,\n",
       " 'I-LOC': 8,\n",
       " 'I-MISC': 7,\n",
       " 'I-ORG': 6,\n",
       " 'I-PER': 4,\n",
       " 'O': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.a.** Load the embeddings for original capitalization of words. If embedding for this word doesn’t exists, associate it with UNKNOWN embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk = gmodel['unk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodel_strategy_1 = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, index in vocab.items():\n",
    "    if word in gmodel:\n",
    "        gmodel_strategy_1[index,:] = gmodel[word]\n",
    "    else:\n",
    "        gmodel_strategy_1[index,:] = unk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.b.** load the embeddings for lowercased capitalization of words. If embedding for this lowercased word doesn’t exists, associate it with UNKNOWN embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodel_strategy_2 = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, index in vocab.items():\n",
    "    word_lower = word.lower()\n",
    "    if word_lower in gmodel:\n",
    "        gmodel_strategy_2[index,:] = gmodel[word_lower]\n",
    "    else:\n",
    "        gmodel_strategy_2[index,:] = unk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.c.** load the embeddings for original capitalization of words. If embedding for this word doesn't exists, try to find the embedding for lowercased version and associate it to the word with original capitalization. Otherwise, associate it with UNKNOWN embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodel_strategy_3 = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, index in vocab.items():\n",
    "    word_lower = word.lower()\n",
    "    if word in gmodel:\n",
    "        gmodel_strategy_3[index,:] = gmodel[word]\n",
    "    elif word_lower in gmodel:\n",
    "        gmodel_strategy_3[index,:] = gmodel[word_lower]\n",
    "    else:\n",
    "        gmodel_strategy_3[index,:] = unk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implement training on batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "def prepare_batch(batch_sentences, vocab):\n",
    "    batch_max_len = max([len(s[0]) for s in batch_sentences])\n",
    "\n",
    "    #prepare a numpy array with the data, initializing the data with 'PAD' \n",
    "    #and all labels with -1; initializing labels to -1 differentiates tokens \n",
    "    #with tags from 'PAD' tokens\n",
    "    batch_data = vocab['pad']*np.ones((len(batch_sentences), batch_max_len))\n",
    "    batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n",
    "\n",
    "    #copy the data to the numpy array\n",
    "    for j in range(len(batch_sentences)):\n",
    "        cur_len = len(batch_sentences[j][0])\n",
    "        batch_data[j][:cur_len] = prepare_sequence(batch_sentences[j][0], vocab)\n",
    "        batch_labels[j][:cur_len] = prepare_sequence(batch_sentences[j][1], tag_vocab)\n",
    "\n",
    "    #since all data are indices, we convert them to torch LongTensors\n",
    "    batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "\n",
    "    return batch_data, batch_labels\n",
    "\n",
    "    \n",
    "# get_batch(train[:10], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_NER(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, embedding_matrix, tagset_size):\n",
    "        super(LSTM_NER, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(embedding_matrix.shape[0], embedding_dim).\\\n",
    "            from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float), freeze=True)            \n",
    "        self.word_embeddings.weight.requires_grad = False\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, bidirectional=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.fc = nn.Linear(2*hidden_dim, tagset_size)\n",
    "        \n",
    "        # field to count epochs\n",
    "        self.epoch_counter = 0\n",
    "        \n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        \n",
    "        lstm_out = lstm_out.view(-1, lstm_out.shape[2])\n",
    "        \n",
    "        fc_out = self.fc(lstm_out)\n",
    "        \n",
    "        tag_scores = F.log_softmax(fc_out, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 100\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def loss_function(outputs, labels):\n",
    "    #reshape labels to give a flat vector of length batch_size*seq_len\n",
    "    labels = labels.view(-1)  \n",
    "\n",
    "    #mask out 'PAD' tokens\n",
    "    mask = (labels >= 0).float()\n",
    "\n",
    "    #the number of tokens is the sum of elements in mask\n",
    "    num_tokens = int(torch.sum(mask).data.item())\n",
    "\n",
    "    #pick the values corresponding to labels and multiply by mask\n",
    "    outputs = outputs[range(outputs.shape[0]), labels]*mask\n",
    "\n",
    "    #cross entropy loss for all non 'PAD' tokens\n",
    "    return -torch.sum(outputs)/num_tokens\n",
    "\n",
    "\n",
    "def get_model(model_strategy):\n",
    "\n",
    "    model = LSTM_NER(EMBEDDING_DIM, HIDDEN_DIM, model_strategy, len(tag_vocab))\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    # check scores before training\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = prepare_batch(train[:BATCH_SIZE], vocab)\n",
    "        tag_scores = model(inputs)\n",
    "        \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_SCORE = 1\n",
    "\n",
    "def get_batch(b_size, data):\n",
    "    batch_sentences = random.sample(data, b_size)\n",
    "    \n",
    "    return prepare_batch(batch_sentences, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, dataset):\n",
    "    \n",
    "    size = len(dataset)\n",
    "    \n",
    "    c_tp = np.zeros(len(tag_vocab), np.int32)\n",
    "    c_fp = np.zeros(len(tag_vocab), np.int32)\n",
    "    c_fn = np.zeros(len(tag_vocab), np.int32)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    tp = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i in range(size):\n",
    "    \n",
    "        sentence_in, targets = prepare_batch(dataset[i:i+1], vocab)\n",
    "\n",
    "        tag_scores = model(sentence_in)\n",
    "        \n",
    "        for k, tag_score in enumerate(tag_scores):\n",
    "            \n",
    "            max_val, prediction = tag_score.max(0)\n",
    "            prediction = prediction.item()\n",
    "            target = targets[0][k].item()\n",
    "                \n",
    "            if prediction == target:\n",
    "                c_tp[target] += 1\n",
    "                tp += 1\n",
    "            else:\n",
    "                c_fp[prediction] += 1\n",
    "                c_fn[target] += 1\n",
    "                \n",
    "                \n",
    "            total += 1\n",
    "        \n",
    "        loss += loss_function(tag_scores, targets).item()\n",
    "        \n",
    "    with np.errstate(all='ignore'):\n",
    "\n",
    "        pr = np.mean(np.nan_to_num(c_tp / (c_tp+c_fp) ))\n",
    "        rc = np.mean(np.nan_to_num(c_tp / (c_tp+c_fn) ))\n",
    "        \n",
    "    f_score = (1 + F_SCORE**2) * pr * rc / (F_SCORE**2 * pr + rc)\n",
    "\n",
    "    return f_score, loss / size, tp/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sorted = sorted(train, key=lambda item: (len(item[0])))\n",
    "\n",
    "def train_model(model, optimizer, epochs = 5):\n",
    "    for epoch in range(epochs):\n",
    "        loss_sum = 0\n",
    "        print(\"Epoch: {}\".format(model.epoch_counter))\n",
    "        model.epoch_counter += 1\n",
    "        for step in tqdm_notebook(range(steps)):\n",
    "            # clear gradients out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "\n",
    "            # sentence_in, targets = get_batch(BATCH_SIZE, train)        \n",
    "            sentence_in, targets = prepare_batch(train_sorted[step * BATCH_SIZE: (step+1) * BATCH_SIZE], vocab)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = model(sentence_in)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum += loss\n",
    "\n",
    "        avg_epoch_loss = np.round((loss_sum / steps).item(), 4)\n",
    "        dev_epoch_f1, dev_epoch_loss, dev_epoch_pr = np.round(validate_model(model, dev), 4)\n",
    "\n",
    "        print(\"\"\"    Train loss:     {}\n",
    "        Validate loss:  {}\n",
    "        Validate F1: {}\n",
    "        Validate Pr: {}\n",
    "    ____________________________________________________________\n",
    "        \"\"\".\\\n",
    "        format(avg_epoch_loss, dev_epoch_loss, dev_epoch_f1, dev_epoch_pr))\n",
    "        \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1, optimizer1 = get_model(gmodel_strategy_1)\n",
    "model2, optimizer2 = get_model(gmodel_strategy_2)\n",
    "model3, optimizer3 = get_model(gmodel_strategy_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e65c4cd00645fab292fbf4313ac8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=220), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Train loss:     0.8734\n",
      "        Validate loss:  1.0532\n",
      "        Validate F1: 0.101\n",
      "        Validate Pr: 0.8325\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926c5833a0754751bbca1168088945b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=220), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Train loss:     0.7403\n",
      "        Validate loss:  0.8992\n",
      "        Validate F1: 0.101\n",
      "        Validate Pr: 0.8325\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea225fa4e4548578ac10fb9bd0f4a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=220), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Train loss:     0.6967\n",
      "        Validate loss:  0.8019\n",
      "        Validate F1: 0.101\n",
      "        Validate Pr: 0.8325\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93612e0778364bebbed78c70ce182944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=220), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Train loss:     0.6765\n",
      "        Validate loss:  0.7632\n",
      "        Validate F1: 0.101\n",
      "        Validate Pr: 0.8325\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d7d468ead2410abebaf4782dd1e97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=220), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Train loss:     0.6685\n",
      "        Validate loss:  0.7455\n",
      "        Validate F1: 0.101\n",
      "        Validate Pr: 0.8325\n",
      "    ____________________________________________________________\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTM_NER(\n",
       "  (word_embeddings): Embedding(30290, 100)\n",
       "  (lstm): LSTM(100, 100, num_layers=2, bidirectional=True)\n",
       "  (fc): Linear(in_features=200, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model1, optimizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.round(validate_model(model1, test), 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
