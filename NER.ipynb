{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efeba752a50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read and process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(name):\n",
    "    f = open(path + name + '.txt', 'r')\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    sentence = []\n",
    "    tag_sentence = []\n",
    "    \n",
    "    for line in lines[2:]:\n",
    "        if line == '\\n':\n",
    "            \n",
    "            result.append((sentence, tag_sentence))\n",
    "            \n",
    "            sentence = []\n",
    "            tag_sentence = []\n",
    "            \n",
    "            continue\n",
    "            \n",
    "        words = line.strip().split()\n",
    "        \n",
    "        sentence.append(words[0])\n",
    "        tag_sentence.append(words[3])\n",
    "            \n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_data('train')\n",
    "dev = get_data('dev')\n",
    "test = get_data('test')\n",
    "\n",
    "dataset = train + dev + test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement 3 strategies for loading the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_name = 'glove.6B.100d.txt'\n",
    "\n",
    "def loadGloveModel(file):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(file,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "gmodel = loadGloveModel(path + embeddings_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "tag_vocab = {}\n",
    "for sent, tags in dataset:\n",
    "    for word in sent:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "    for tag in tags:\n",
    "        if tag not in tag_vocab:\n",
    "            tag_vocab[tag] = len(tag_vocab)\n",
    "\n",
    "vocab['pad'] = len(vocab)\n",
    "            \n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC': 5,\n",
       " 'B-MISC': 2,\n",
       " 'B-ORG': 0,\n",
       " 'B-PER': 3,\n",
       " 'I-LOC': 8,\n",
       " 'I-MISC': 7,\n",
       " 'I-ORG': 6,\n",
       " 'I-PER': 4,\n",
       " 'O': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.a.** Load the embeddings for original capitalization of words. If embedding for this word doesn’t exists, associate it with UNKNOWN embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk = gmodel['unk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodel_strategy_1 = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, index in vocab.items():\n",
    "    if word in gmodel:\n",
    "        gmodel_strategy_1[index,:] = gmodel[word]\n",
    "    else:\n",
    "        gmodel_strategy_1[index,:] = unk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.b.** load the embeddings for lowercased capitalization of words. If embedding for this lowercased word doesn’t exists, associate it with UNKNOWN embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodel_strategy_2 = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, index in vocab.items():\n",
    "    word_lower = word.lower()\n",
    "    if word_lower in gmodel:\n",
    "        gmodel_strategy_2[index,:] = gmodel[word_lower]\n",
    "    else:\n",
    "        gmodel_strategy_2[index,:] = unk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.c.** load the embeddings for original capitalization of words. If embedding for this word doesn't exists, try to find the embedding for lowercased version and associate it to the word with original capitalization. Otherwise, associate it with UNKNOWN embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodel_strategy_3 = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, index in vocab.items():\n",
    "    word_lower = word.lower()\n",
    "    if word in gmodel:\n",
    "        gmodel_strategy_3[index,:] = gmodel[word]\n",
    "    elif word_lower in gmodel:\n",
    "        gmodel_strategy_3[index,:] = gmodel[word_lower]\n",
    "    else:\n",
    "        gmodel_strategy_3[index,:] = unk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implement training on batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "def prepare_batch(batch_sentences, vocab):\n",
    "    batch_max_len = max([len(s[0]) for s in batch_sentences])\n",
    "\n",
    "    #prepare a numpy array with the data, initializing the data with 'PAD' \n",
    "    #and all labels with -1; initializing labels to -1 differentiates tokens \n",
    "    #with tags from 'PAD' tokens\n",
    "    batch_data = vocab['pad']*np.ones((len(batch_sentences), batch_max_len))\n",
    "    batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n",
    "\n",
    "    #copy the data to the numpy array\n",
    "    for j in range(len(batch_sentences)):\n",
    "        cur_len = len(batch_sentences[j][0])\n",
    "        batch_data[j][:cur_len] = prepare_sequence(batch_sentences[j][0], vocab)\n",
    "        batch_labels[j][:cur_len] = prepare_sequence(batch_sentences[j][1], tag_vocab)\n",
    "\n",
    "    #since all data are indices, we convert them to torch LongTensors\n",
    "    batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "\n",
    "    #convert Tensors to Variables\n",
    "#     batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "    return batch_data, batch_labels\n",
    "\n",
    "    \n",
    "# get_batch(train[:10], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_NER(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, embedding_matrix, tagset_size):\n",
    "        super(LSTM_NER, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(embedding_matrix.shape[0], embedding_dim).\\\n",
    "            from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))            \n",
    "        self.word_embeddings.weight.requires_grad = False\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        # self.word_embeddings = nn.Embedding(embedding_matrix.shape[0], embedding_dim)\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        # self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "        \n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        \n",
    "        # lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        \n",
    "        lstm_out = lstm_out.view(-1, lstm_out.shape[2])\n",
    "        \n",
    "        fc_out = self.fc(lstm_out)\n",
    "        \n",
    "        tag_scores = F.log_softmax(fc_out, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1248, -2.1080, -2.2440,  ..., -2.1052, -2.2949, -2.2587],\n",
      "        [-2.1333, -2.1313, -2.2185,  ..., -2.1396, -2.2498, -2.2420],\n",
      "        [-2.1532, -2.0811, -2.2415,  ..., -2.1071, -2.2235, -2.2708],\n",
      "        ...,\n",
      "        [-2.2264, -2.1293, -2.2512,  ..., -2.1383, -2.1746, -2.1226],\n",
      "        [-2.2264, -2.1293, -2.2512,  ..., -2.1383, -2.1746, -2.1226],\n",
      "        [-2.2264, -2.1293, -2.2512,  ..., -2.1383, -2.1746, -2.1226]])\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = 100\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "model = LSTM_NER(EMBEDDING_DIM, HIDDEN_DIM, gmodel_strategy_3, len(tag_vocab))\n",
    "# loss_function = nn.NLLLoss()\n",
    "\n",
    "def loss_function(outputs, labels):\n",
    "    #reshape labels to give a flat vector of length batch_size*seq_len\n",
    "    labels = labels.view(-1)  \n",
    "\n",
    "    #mask out 'PAD' tokens\n",
    "    mask = (labels >= 0).float()\n",
    "\n",
    "    #the number of tokens is the sum of elements in mask\n",
    "    num_tokens = int(torch.sum(mask).data.item())\n",
    "\n",
    "    #pick the values corresponding to labels and multiply by mask\n",
    "    outputs = outputs[range(outputs.shape[0]), labels]*mask\n",
    "\n",
    "    #cross entropy loss for all non 'PAD' tokens\n",
    "    return -torch.sum(outputs)/num_tokens\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "#     inputs = prepare_sequence(train[0][0], vocab)\n",
    "    inputs, labels = prepare_batch(train[:BATCH_SIZE], vocab)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(b_size, data):\n",
    "    batch_sentences = random.sample(data, b_size)\n",
    "    \n",
    "    return prepare_batch(batch_sentences, vocab)\n",
    "\n",
    "# get_batch(BATCH_SIZE, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.2835, grad_fn=<DivBackward0>)\n",
      "1 tensor(0.3101, grad_fn=<DivBackward0>)\n",
      "2 tensor(0.2777, grad_fn=<DivBackward0>)\n",
      "3 tensor(0.2138, grad_fn=<DivBackward0>)\n",
      "4 tensor(0.2865, grad_fn=<DivBackward0>)\n",
      "5 tensor(0.2663, grad_fn=<DivBackward0>)\n",
      "6 tensor(0.2909, grad_fn=<DivBackward0>)\n",
      "7 tensor(0.3198, grad_fn=<DivBackward0>)\n",
      "8 tensor(0.2459, grad_fn=<DivBackward0>)\n",
      "9 tensor(0.2133, grad_fn=<DivBackward0>)\n",
      "10 tensor(0.2445, grad_fn=<DivBackward0>)\n",
      "11 tensor(0.2432, grad_fn=<DivBackward0>)\n",
      "12 tensor(0.2574, grad_fn=<DivBackward0>)\n",
      "13 tensor(0.2742, grad_fn=<DivBackward0>)\n",
      "14 tensor(0.2537, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "steps = len(train) // BATCH_SIZE + 1\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "for epoch in range(15):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for step in range(steps):\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        \n",
    "        # sentence_in = prepare_sequence(sentence, vocab)\n",
    "        # targets = prepare_sequence(tags, tag_vocab)\n",
    "        \n",
    "        sentence_in, targets = get_batch(BATCH_SIZE, train)        \n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(epoch, loss)\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "#     inputs = prepare_sequence(train[0][0], vocab)\n",
    "#     tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    \n",
    "#     print(tag_scores)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, dev, vocab, tag_vocab):\n",
    "    \n",
    "    size = len(dev)\n",
    "    \n",
    "    sentence_in, targets = get_batch(size, dev)\n",
    "    \n",
    "    prediction = model(sentence_in)\n",
    "    \n",
    "    return prediction, targets\n",
    "\n",
    "prediction, target = validate_model(model, test, vocab, tag_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8143147969570341\n"
     ]
    }
   ],
   "source": [
    "index_global = 0\n",
    "counter = 0\n",
    "for row in target:\n",
    "    for col in row:        \n",
    "        if col < 0:\n",
    "            continue\n",
    "            \n",
    "        result = col\n",
    "        \n",
    "        max_val, pred = prediction[ index_global ].max(0)\n",
    "        \n",
    "        if pred == result:\n",
    "            counter += 1\n",
    "        \n",
    "        index_global += 1\n",
    "\n",
    "    \n",
    "print(counter / index_global )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
