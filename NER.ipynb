{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdfd817ca50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read and process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(name):\n",
    "    f = open(path + name + '.txt', 'r')\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    sentence = []\n",
    "    tag_sentence = []\n",
    "    \n",
    "    for line in lines[2:]:\n",
    "        if line == '\\n':           \n",
    "            \n",
    "            if len(sentence) > 0:\n",
    "                result.append((sentence, tag_sentence))\n",
    "            \n",
    "                sentence = []\n",
    "                tag_sentence = []\n",
    "            \n",
    "            continue\n",
    "            \n",
    "        words = line.strip().split()\n",
    "        \n",
    "        if words[0] == '-DOCSTART-':\n",
    "            continue\n",
    "        \n",
    "        sentence.append(words[0])\n",
    "        tag_sentence.append(words[3])\n",
    "            \n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_data('train')\n",
    "dev = get_data('dev')\n",
    "test = get_data('test')\n",
    "\n",
    "dataset = train + dev + test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement 3 strategies for loading the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_name = 'glove.6B.100d.txt'\n",
    "\n",
    "def loadGloveModel(file):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(file,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "gmodel = loadGloveModel(path + embeddings_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "tag_vocab = {}\n",
    "for sent, tags in dataset:\n",
    "    for word in sent:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "    for tag in tags:\n",
    "        if tag not in tag_vocab:\n",
    "            tag_vocab[tag] = len(tag_vocab)\n",
    "\n",
    "vocab['pad'] = len(vocab)\n",
    "            \n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC': 5,\n",
       " 'B-MISC': 2,\n",
       " 'B-ORG': 0,\n",
       " 'B-PER': 3,\n",
       " 'I-LOC': 8,\n",
       " 'I-MISC': 7,\n",
       " 'I-ORG': 6,\n",
       " 'I-PER': 4,\n",
       " 'O': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.a.** Load the embeddings for original capitalization of words. If embedding for this word doesn’t exists, associate it with UNKNOWN embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk = gmodel['unk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodel_strategy_1 = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, index in vocab.items():\n",
    "    if word in gmodel:\n",
    "        gmodel_strategy_1[index,:] = gmodel[word]\n",
    "    else:\n",
    "        gmodel_strategy_1[index,:] = unk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.b.** load the embeddings for lowercased capitalization of words. If embedding for this lowercased word doesn’t exists, associate it with UNKNOWN embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodel_strategy_2 = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, index in vocab.items():\n",
    "    word_lower = word.lower()\n",
    "    if word_lower in gmodel:\n",
    "        gmodel_strategy_2[index,:] = gmodel[word_lower]\n",
    "    else:\n",
    "        gmodel_strategy_2[index,:] = unk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.c.** load the embeddings for original capitalization of words. If embedding for this word doesn't exists, try to find the embedding for lowercased version and associate it to the word with original capitalization. Otherwise, associate it with UNKNOWN embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodel_strategy_3 = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, index in vocab.items():\n",
    "    word_lower = word.lower()\n",
    "    if word in gmodel:\n",
    "        gmodel_strategy_3[index,:] = gmodel[word]\n",
    "    elif word_lower in gmodel:\n",
    "        gmodel_strategy_3[index,:] = gmodel[word_lower]\n",
    "    else:\n",
    "        gmodel_strategy_3[index,:] = unk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implement training on batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "def prepare_batch(batch_sentences, vocab):\n",
    "    batch_max_len = max([len(s[0]) for s in batch_sentences])\n",
    "\n",
    "    # prepare a numpy array with the data, initializing the data with 'PAD' \n",
    "    # and all labels with -1; initializing labels to -1 differentiates tokens \n",
    "    # with tags from 'PAD' tokens\n",
    "    batch_data = vocab['pad']*np.ones((len(batch_sentences), batch_max_len))\n",
    "    batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n",
    "\n",
    "    # copy the data to the numpy array\n",
    "    for j in range(len(batch_sentences)):\n",
    "        cur_len = len(batch_sentences[j][0])\n",
    "        batch_data[j][:cur_len] = prepare_sequence(batch_sentences[j][0], vocab)\n",
    "        batch_labels[j][:cur_len] = prepare_sequence(batch_sentences[j][1], tag_vocab)\n",
    "\n",
    "    # convert data them to torch LongTensors\n",
    "    batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "\n",
    "    return batch_data, batch_labels\n",
    "\n",
    "    \n",
    "# get_batch(train[:10], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class witch creates NER model\n",
    "class LSTM_NER(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, embedding_matrix, tagset_size):\n",
    "        super(LSTM_NER, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(embedding_matrix.shape[0], embedding_dim).\\\n",
    "            from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float), freeze=True)            \n",
    "        self.word_embeddings.weight.requires_grad = False\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, bidirectional=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.fc = nn.Linear(2*hidden_dim, tagset_size)\n",
    "        \n",
    "        # field to count epochs\n",
    "        self.epoch_counter = 0\n",
    "        \n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        \n",
    "        lstm_out = lstm_out.view(-1, lstm_out.shape[2])\n",
    "        \n",
    "        fc_out = self.fc(lstm_out)\n",
    "        \n",
    "        tag_scores = F.log_softmax(fc_out, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension of hidden layer\n",
    "HIDDEN_DIM = 100\n",
    "\n",
    "# batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "def loss_function(outputs, labels):\n",
    "    #reshape labels to give a flat vector of length batch_size*seq_len\n",
    "    labels = labels.view(-1)  \n",
    "\n",
    "    #mask out 'PAD' tokens\n",
    "    mask = (labels >= 0).float()\n",
    "\n",
    "    #the number of tokens is the sum of elements in mask\n",
    "    num_tokens = int(torch.sum(mask).data.item())\n",
    "\n",
    "    #pick the values corresponding to labels and multiply by mask\n",
    "    outputs = outputs[range(outputs.shape[0]), labels]*mask\n",
    "\n",
    "    #cross entropy loss for all non 'PAD' tokens\n",
    "    return -torch.sum(outputs)/num_tokens\n",
    "\n",
    "\n",
    "# create model with corresponded embeddings strategy\n",
    "def get_model(model_strategy):\n",
    "\n",
    "    model = LSTM_NER(EMBEDDING_DIM, HIDDEN_DIM, model_strategy, len(tag_vocab))\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    # check scores before training\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = prepare_batch(train[:BATCH_SIZE], vocab)\n",
    "        tag_scores = model(inputs)\n",
    "        \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I checked different methods how to take batches.\n",
    "# This one take random dubset from dataset\n",
    "def get_batch(b_size, data):\n",
    "    batch_sentences = random.sample(data, b_size)\n",
    "    \n",
    "    return prepare_batch(batch_sentences, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement the calculation of token-level Precision / Recall / F1 / F0.5 scores for all classes in average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I compared 2 approaches (micro- and macro- average) for model evaluation. As we have one big tag-class ('O') and model very often predicts tags as 'O', micro-average F-score always gives good enough result even if the model started training and always returns 'O'.\n",
    "\n",
    "After that I added a macro-average approach to take into account each tag-class equivalent.\n",
    "\n",
    "The function below returns F1 macro and micro scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_SCORE = 1\n",
    "\n",
    "# return F score, loss and global precision\n",
    "def validate_model(model, dataset):\n",
    "    \n",
    "    size = len(dataset)\n",
    "    \n",
    "    # init counter for TP, FP, FN and each tag-class\n",
    "    c_tp = np.zeros(len(tag_vocab), np.int32)\n",
    "    c_fp = np.zeros(len(tag_vocab), np.int32)\n",
    "    c_fn = np.zeros(len(tag_vocab), np.int32)\n",
    "    \n",
    "    # total loss\n",
    "    loss = 0\n",
    "    \n",
    "    # calculate global precision\n",
    "    tp = 0\n",
    "    total = 0\n",
    "    \n",
    "    # validate each sentence of the test set separately\n",
    "    for i in range(size):    \n",
    "        sentence_in, targets = prepare_batch(dataset[i:i+1], vocab)\n",
    "\n",
    "        # model scores\n",
    "        tag_scores = model(sentence_in)\n",
    "        \n",
    "        # take each word\n",
    "        for k, tag_score in enumerate(tag_scores):\n",
    "            \n",
    "            # the index of the max value corresponds to the index of tag in global tag_vocab\n",
    "            max_val, prediction = tag_score.max(0)\n",
    "            prediction = prediction.item()\n",
    "            \n",
    "            # labeled tag value\n",
    "            target = targets[0][k].item()\n",
    "                \n",
    "            # if tag is predicted correctly\n",
    "            if prediction == target:\n",
    "                # increase TP value for corresponded tag\n",
    "                c_tp[target] += 1\n",
    "                # increase global TP value\n",
    "                tp += 1\n",
    "            else:\n",
    "                # increase FP for predicted tag\n",
    "                c_fp[prediction] += 1\n",
    "                # increase FN for true (target) tag\n",
    "                c_fn[target] += 1\n",
    "            \n",
    "            total += 1\n",
    "        \n",
    "        # increase loss function\n",
    "        loss += loss_function(tag_scores, targets).item()\n",
    "        \n",
    "    # to prevent warning of dividing by zero\n",
    "    with np.errstate(all='ignore'):\n",
    "\n",
    "        # take macro average for precision and recall\n",
    "        pr = np.mean(np.nan_to_num(c_tp / (c_tp+c_fp) ))\n",
    "        rc = np.mean(np.nan_to_num(c_tp / (c_tp+c_fn) ))\n",
    "        \n",
    "    # calculate F score\n",
    "    f_score_macro = (1 + F_SCORE**2) * pr * rc / (F_SCORE**2 * pr + rc)\n",
    "    f_score_micro = tp / total\n",
    "\n",
    "    return f_score_macro, f_score_micro, loss / size\n",
    "\n",
    "# validate_model(model3, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I decided to sort train set by sentence length to minimize number of paddings in the batch. It increased the result significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sorted = sorted(train, key=lambda item: (len(item[0])))\n",
    "\n",
    "def train_model(model, optimizer, epochs = 5):\n",
    "    \n",
    "    steps = len(train) // BATCH_SIZE + 1\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss_sum = 0\n",
    "        print(\"Epoch: {}\".format(model.epoch_counter))\n",
    "        model.epoch_counter += 1\n",
    "        # for step in tqdm_notebook(range(steps)):\n",
    "        for step in range(steps):\n",
    "            # clear gradients out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "\n",
    "            # sentence_in, targets = get_batch(BATCH_SIZE, train)        \n",
    "            sentence_in, targets = prepare_batch(train_sorted[step * BATCH_SIZE: (step+1) * BATCH_SIZE], vocab)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = model(sentence_in)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum += loss\n",
    "\n",
    "        avg_epoch_loss = np.round((loss_sum / steps).item(), 4)\n",
    "        dev_epoch_f1_macro, dev_epoch_f1_micro, dev_epoch_loss = np.round(validate_model(model, dev), 4)\n",
    "\n",
    "        print(\"\"\"    Train loss       :{}\n",
    "    Validation loss  :{}\n",
    "    Validation F1 macro :{}\n",
    "    Validation F1 micro :{}\n",
    "    ____________________________________________________________\n",
    "        \"\"\".\\\n",
    "        format(avg_epoch_loss, dev_epoch_loss, dev_epoch_f1_macro, dev_epoch_f1_micro))\n",
    "        \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1, optimizer1 = get_model(gmodel_strategy_1)\n",
    "model2, optimizer2 = get_model(gmodel_strategy_2)\n",
    "model3, optimizer3 = get_model(gmodel_strategy_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "    Train loss       :0.8805\n",
      "    Validation loss  :1.0424\n",
      "    Validation F1 macro :0.101\n",
      "    Validation F1 micro :0.8325\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 1\n",
      "    Train loss       :0.741\n",
      "    Validation loss  :0.8954\n",
      "    Validation F1 macro :0.101\n",
      "    Validation F1 micro :0.8325\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 2\n",
      "    Train loss       :0.6961\n",
      "    Validation loss  :0.7991\n",
      "    Validation F1 macro :0.101\n",
      "    Validation F1 micro :0.8325\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 3\n",
      "    Train loss       :0.6752\n",
      "    Validation loss  :0.7601\n",
      "    Validation F1 macro :0.101\n",
      "    Validation F1 micro :0.8325\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 4\n",
      "    Train loss       :0.6668\n",
      "    Validation loss  :0.7426\n",
      "    Validation F1 macro :0.101\n",
      "    Validation F1 micro :0.8325\n",
      "    ____________________________________________________________\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "train_model(model1, optimizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "    Train loss       :0.9203\n",
      "    Validation loss  :1.1994\n",
      "    Validation F1 macro :0.101\n",
      "    Validation F1 micro :0.8325\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 1\n",
      "    Train loss       :0.737\n",
      "    Validation loss  :1.0403\n",
      "    Validation F1 macro :0.1731\n",
      "    Validation F1 micro :0.8392\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 2\n",
      "    Train loss       :0.6512\n",
      "    Validation loss  :0.841\n",
      "    Validation F1 macro :0.2242\n",
      "    Validation F1 micro :0.8464\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 3\n",
      "    Train loss       :0.5799\n",
      "    Validation loss  :0.7114\n",
      "    Validation F1 macro :0.2642\n",
      "    Validation F1 micro :0.8631\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 4\n",
      "    Train loss       :0.5225\n",
      "    Validation loss  :0.6388\n",
      "    Validation F1 macro :0.2841\n",
      "    Validation F1 micro :0.8736\n",
      "    ____________________________________________________________\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "train_model(model2, optimizer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "    Train loss       :0.9237\n",
      "    Validation loss  :1.1852\n",
      "    Validation F1 macro :0.101\n",
      "    Validation F1 micro :0.8325\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 1\n",
      "    Train loss       :0.7388\n",
      "    Validation loss  :1.0374\n",
      "    Validation F1 macro :0.1749\n",
      "    Validation F1 micro :0.8409\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 2\n",
      "    Train loss       :0.6589\n",
      "    Validation loss  :0.8439\n",
      "    Validation F1 macro :0.1938\n",
      "    Validation F1 micro :0.8478\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 3\n",
      "    Train loss       :0.5922\n",
      "    Validation loss  :0.7165\n",
      "    Validation F1 macro :0.2604\n",
      "    Validation F1 micro :0.8642\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 4\n",
      "    Train loss       :0.5359\n",
      "    Validation loss  :0.6436\n",
      "    Validation F1 macro :0.2789\n",
      "    Validation F1 micro :0.8722\n",
      "    ____________________________________________________________\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "train_model(model3, optimizer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Provide the report the performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the training process above, **model1**, which corresponds to the embedding building strategy 1, does not increase F1 scores despite of the decreasing the loss function.\n",
    "\n",
    "**model2** and **model3** show very similar training progress but the last one is slightly better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare results on the **test** set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1005, 0.8253, 0.7863])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(validate_model(model1, test), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2805, 0.8679, 0.6471])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(validate_model(model2, test), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2816, 0.8693, 0.6512])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(validate_model(model3, test), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best result is shown by **model3**. F1 micro score is **86.9%** and F1 macro score is **28.2%**.\n",
    "It looks like **model1** always return tag value 'O' as **82.5%** is F1 micro score corresponds to the number of 'O' tags in the dataset.\n",
    "\n",
    "One more important thing is freezing embedding vectors for training. Firstly, it did not show improvement and, secondly, it requires a lot of memory for calculating gradients and I can not keep 3 models in the memory simultaneously. Along with it, it increases training time.\n",
    "\n",
    "Let's continue **model3** training and see the best possible result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6004, 0.9139, 0.3395])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(validate_model(model3, test), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I run 70 epochs more and achieved F1 macro score **60%** and F1 micro score **91.4%**. The training process provided below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "    Train loss       :0.4925\n",
      "    Validation loss  :0.5967\n",
      "    Validation F1 macro :0.2943\n",
      "    Validation F1 micro :0.8793\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 6\n",
      "    Train loss       :0.4599\n",
      "    Validation loss  :0.562\n",
      "    Validation F1 macro :0.3\n",
      "    Validation F1 micro :0.8833\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 7\n",
      "    Train loss       :0.4338\n",
      "    Validation loss  :0.5345\n",
      "    Validation F1 macro :0.3353\n",
      "    Validation F1 micro :0.8851\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 8\n",
      "    Train loss       :0.4114\n",
      "    Validation loss  :0.5119\n",
      "    Validation F1 macro :0.345\n",
      "    Validation F1 micro :0.8899\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 9\n",
      "    Train loss       :0.3919\n",
      "    Validation loss  :0.4932\n",
      "    Validation F1 macro :0.3563\n",
      "    Validation F1 micro :0.8937\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 10\n",
      "    Train loss       :0.3747\n",
      "    Validation loss  :0.4773\n",
      "    Validation F1 macro :0.3578\n",
      "    Validation F1 micro :0.8949\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 11\n",
      "    Train loss       :0.3596\n",
      "    Validation loss  :0.4634\n",
      "    Validation F1 macro :0.4122\n",
      "    Validation F1 micro :0.8969\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 12\n",
      "    Train loss       :0.346\n",
      "    Validation loss  :0.4512\n",
      "    Validation F1 macro :0.4206\n",
      "    Validation F1 micro :0.8994\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 13\n",
      "    Train loss       :0.3337\n",
      "    Validation loss  :0.4402\n",
      "    Validation F1 macro :0.4277\n",
      "    Validation F1 micro :0.9023\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 14\n",
      "    Train loss       :0.3224\n",
      "    Validation loss  :0.4303\n",
      "    Validation F1 macro :0.4377\n",
      "    Validation F1 micro :0.9054\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 15\n",
      "    Train loss       :0.3117\n",
      "    Validation loss  :0.4213\n",
      "    Validation F1 macro :0.4432\n",
      "    Validation F1 micro :0.9072\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 16\n",
      "    Train loss       :0.3015\n",
      "    Validation loss  :0.4132\n",
      "    Validation F1 macro :0.4508\n",
      "    Validation F1 micro :0.9095\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 17\n",
      "    Train loss       :0.2918\n",
      "    Validation loss  :0.406\n",
      "    Validation F1 macro :0.4537\n",
      "    Validation F1 micro :0.9111\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 18\n",
      "    Train loss       :0.2829\n",
      "    Validation loss  :0.3996\n",
      "    Validation F1 macro :0.4547\n",
      "    Validation F1 micro :0.9113\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 19\n",
      "    Train loss       :0.2748\n",
      "    Validation loss  :0.3938\n",
      "    Validation F1 macro :0.4578\n",
      "    Validation F1 micro :0.9121\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 20\n",
      "    Train loss       :0.2676\n",
      "    Validation loss  :0.3887\n",
      "    Validation F1 macro :0.4758\n",
      "    Validation F1 micro :0.9129\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 21\n",
      "    Train loss       :0.2616\n",
      "    Validation loss  :0.3841\n",
      "    Validation F1 macro :0.4769\n",
      "    Validation F1 micro :0.9132\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 22\n",
      "    Train loss       :0.2565\n",
      "    Validation loss  :0.3801\n",
      "    Validation F1 macro :0.4918\n",
      "    Validation F1 micro :0.9137\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 23\n",
      "    Train loss       :0.2517\n",
      "    Validation loss  :0.3765\n",
      "    Validation F1 macro :0.493\n",
      "    Validation F1 micro :0.9141\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 24\n",
      "    Train loss       :0.2464\n",
      "    Validation loss  :0.3731\n",
      "    Validation F1 macro :0.4929\n",
      "    Validation F1 micro :0.9144\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 25\n",
      "    Train loss       :0.241\n",
      "    Validation loss  :0.3701\n",
      "    Validation F1 macro :0.4825\n",
      "    Validation F1 micro :0.9145\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 26\n",
      "    Train loss       :0.2372\n",
      "    Validation loss  :0.3678\n",
      "    Validation F1 macro :0.4853\n",
      "    Validation F1 micro :0.9151\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 27\n",
      "    Train loss       :0.2341\n",
      "    Validation loss  :0.3654\n",
      "    Validation F1 macro :0.4842\n",
      "    Validation F1 micro :0.915\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 28\n",
      "    Train loss       :0.2291\n",
      "    Validation loss  :0.3627\n",
      "    Validation F1 macro :0.4842\n",
      "    Validation F1 micro :0.915\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 29\n",
      "    Train loss       :0.2257\n",
      "    Validation loss  :0.3606\n",
      "    Validation F1 macro :0.4866\n",
      "    Validation F1 micro :0.9156\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 30\n",
      "    Train loss       :0.2313\n",
      "    Validation loss  :0.3593\n",
      "    Validation F1 macro :0.4923\n",
      "    Validation F1 micro :0.9164\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 31\n",
      "    Train loss       :0.2207\n",
      "    Validation loss  :0.356\n",
      "    Validation F1 macro :0.4924\n",
      "    Validation F1 micro :0.9166\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 32\n",
      "    Train loss       :0.2179\n",
      "    Validation loss  :0.3533\n",
      "    Validation F1 macro :0.4891\n",
      "    Validation F1 micro :0.9167\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 33\n",
      "    Train loss       :0.2142\n",
      "    Validation loss  :0.3507\n",
      "    Validation F1 macro :0.4933\n",
      "    Validation F1 micro :0.9172\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 34\n",
      "    Train loss       :0.2115\n",
      "    Validation loss  :0.3485\n",
      "    Validation F1 macro :0.4984\n",
      "    Validation F1 micro :0.9176\n",
      "    ____________________________________________________________\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "train_model(model3, optimizer3, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35\n",
      "    Train loss       :0.2085\n",
      "    Validation loss  :0.3464\n",
      "    Validation F1 macro :0.4978\n",
      "    Validation F1 micro :0.9173\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 36\n",
      "    Train loss       :0.2057\n",
      "    Validation loss  :0.3446\n",
      "    Validation F1 macro :0.4995\n",
      "    Validation F1 micro :0.9177\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 37\n",
      "    Train loss       :0.203\n",
      "    Validation loss  :0.343\n",
      "    Validation F1 macro :0.4998\n",
      "    Validation F1 micro :0.9179\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 38\n",
      "    Train loss       :0.2013\n",
      "    Validation loss  :0.3418\n",
      "    Validation F1 macro :0.5003\n",
      "    Validation F1 micro :0.918\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 39\n",
      "    Train loss       :0.199\n",
      "    Validation loss  :0.3408\n",
      "    Validation F1 macro :0.5014\n",
      "    Validation F1 micro :0.9184\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 40\n",
      "    Train loss       :0.1972\n",
      "    Validation loss  :0.3396\n",
      "    Validation F1 macro :0.5002\n",
      "    Validation F1 micro :0.9184\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 41\n",
      "    Train loss       :0.1948\n",
      "    Validation loss  :0.3376\n",
      "    Validation F1 macro :0.5072\n",
      "    Validation F1 micro :0.9197\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 42\n",
      "    Train loss       :0.1917\n",
      "    Validation loss  :0.3355\n",
      "    Validation F1 macro :0.5078\n",
      "    Validation F1 micro :0.9198\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 43\n",
      "    Train loss       :0.1912\n",
      "    Validation loss  :0.3335\n",
      "    Validation F1 macro :0.5273\n",
      "    Validation F1 micro :0.92\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 44\n",
      "    Train loss       :0.1901\n",
      "    Validation loss  :0.3315\n",
      "    Validation F1 macro :0.5293\n",
      "    Validation F1 micro :0.9203\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 45\n",
      "    Train loss       :0.1872\n",
      "    Validation loss  :0.3306\n",
      "    Validation F1 macro :0.5332\n",
      "    Validation F1 micro :0.9206\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 46\n",
      "    Train loss       :0.1854\n",
      "    Validation loss  :0.3295\n",
      "    Validation F1 macro :0.5315\n",
      "    Validation F1 micro :0.9211\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 47\n",
      "    Train loss       :0.1834\n",
      "    Validation loss  :0.3288\n",
      "    Validation F1 macro :0.5346\n",
      "    Validation F1 micro :0.9211\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 48\n",
      "    Train loss       :0.181\n",
      "    Validation loss  :0.3273\n",
      "    Validation F1 macro :0.5392\n",
      "    Validation F1 micro :0.9216\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 49\n",
      "    Train loss       :0.1923\n",
      "    Validation loss  :0.3258\n",
      "    Validation F1 macro :0.5419\n",
      "    Validation F1 micro :0.9223\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 50\n",
      "    Train loss       :0.184\n",
      "    Validation loss  :0.3245\n",
      "    Validation F1 macro :0.5402\n",
      "    Validation F1 micro :0.9219\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 51\n",
      "    Train loss       :0.1783\n",
      "    Validation loss  :0.3236\n",
      "    Validation F1 macro :0.54\n",
      "    Validation F1 micro :0.9219\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 52\n",
      "    Train loss       :0.1766\n",
      "    Validation loss  :0.3229\n",
      "    Validation F1 macro :0.5403\n",
      "    Validation F1 micro :0.922\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 53\n",
      "    Train loss       :0.1743\n",
      "    Validation loss  :0.3216\n",
      "    Validation F1 macro :0.5424\n",
      "    Validation F1 micro :0.9225\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 54\n",
      "    Train loss       :0.1723\n",
      "    Validation loss  :0.32\n",
      "    Validation F1 macro :0.5425\n",
      "    Validation F1 micro :0.9225\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 55\n",
      "    Train loss       :0.1724\n",
      "    Validation loss  :0.3189\n",
      "    Validation F1 macro :0.5945\n",
      "    Validation F1 micro :0.9235\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 56\n",
      "    Train loss       :0.1696\n",
      "    Validation loss  :0.3179\n",
      "    Validation F1 macro :0.5923\n",
      "    Validation F1 micro :0.9238\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 57\n",
      "    Train loss       :0.1682\n",
      "    Validation loss  :0.3171\n",
      "    Validation F1 macro :0.5883\n",
      "    Validation F1 micro :0.9238\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 58\n",
      "    Train loss       :0.1657\n",
      "    Validation loss  :0.3159\n",
      "    Validation F1 macro :0.588\n",
      "    Validation F1 micro :0.9238\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 59\n",
      "    Train loss       :0.1644\n",
      "    Validation loss  :0.3147\n",
      "    Validation F1 macro :0.5929\n",
      "    Validation F1 micro :0.924\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 60\n",
      "    Train loss       :0.1623\n",
      "    Validation loss  :0.3137\n",
      "    Validation F1 macro :0.5887\n",
      "    Validation F1 micro :0.924\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 61\n",
      "    Train loss       :0.1635\n",
      "    Validation loss  :0.3132\n",
      "    Validation F1 macro :0.5981\n",
      "    Validation F1 micro :0.9247\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 62\n",
      "    Train loss       :0.1603\n",
      "    Validation loss  :0.3122\n",
      "    Validation F1 macro :0.6011\n",
      "    Validation F1 micro :0.9248\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 63\n",
      "    Train loss       :0.1589\n",
      "    Validation loss  :0.3117\n",
      "    Validation F1 macro :0.6097\n",
      "    Validation F1 micro :0.9251\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 64\n",
      "    Train loss       :0.1575\n",
      "    Validation loss  :0.3108\n",
      "    Validation F1 macro :0.6099\n",
      "    Validation F1 micro :0.9257\n",
      "    ____________________________________________________________\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "train_model(model3, optimizer3, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65\n",
      "    Train loss       :0.1568\n",
      "    Validation loss  :0.3097\n",
      "    Validation F1 macro :0.6105\n",
      "    Validation F1 micro :0.9255\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 66\n",
      "    Train loss       :0.1583\n",
      "    Validation loss  :0.3093\n",
      "    Validation F1 macro :0.6186\n",
      "    Validation F1 micro :0.9258\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 67\n",
      "    Train loss       :0.1538\n",
      "    Validation loss  :0.3086\n",
      "    Validation F1 macro :0.6176\n",
      "    Validation F1 micro :0.9258\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 68\n",
      "    Train loss       :0.1545\n",
      "    Validation loss  :0.3077\n",
      "    Validation F1 macro :0.6174\n",
      "    Validation F1 micro :0.9259\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 69\n",
      "    Train loss       :0.1525\n",
      "    Validation loss  :0.307\n",
      "    Validation F1 macro :0.6179\n",
      "    Validation F1 micro :0.9259\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 70\n",
      "    Train loss       :0.1495\n",
      "    Validation loss  :0.3065\n",
      "    Validation F1 macro :0.6178\n",
      "    Validation F1 micro :0.926\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 71\n",
      "    Train loss       :0.1501\n",
      "    Validation loss  :0.3057\n",
      "    Validation F1 macro :0.6181\n",
      "    Validation F1 micro :0.926\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 72\n",
      "    Train loss       :0.1518\n",
      "    Validation loss  :0.3052\n",
      "    Validation F1 macro :0.6179\n",
      "    Validation F1 micro :0.9258\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 73\n",
      "    Train loss       :0.1462\n",
      "    Validation loss  :0.3046\n",
      "    Validation F1 macro :0.6144\n",
      "    Validation F1 micro :0.9257\n",
      "    ____________________________________________________________\n",
      "        \n",
      "Epoch: 74\n",
      "    Train loss       :0.1446\n",
      "    Validation loss  :0.3042\n",
      "    Validation F1 macro :0.6122\n",
      "    Validation F1 micro :0.9258\n",
      "    ____________________________________________________________\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "train_model(model3, optimizer3, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6004, 0.9139, 0.3395])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(validate_model(model3, test), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "There si a list of tutorials I used while working on this task\n",
    "\n",
    "1. https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#sphx-glr-beginner-nlp-sequence-models-tutorial-py\n",
    "\n",
    "2. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
